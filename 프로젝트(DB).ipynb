{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 번째 페이지\n",
      "2 번째 페이지\n",
      "3 번째 페이지\n",
      "4 번째 페이지\n",
      "5 번째 페이지\n",
      "6 번째 페이지\n",
      "7 번째 페이지\n",
      "8 번째 페이지\n",
      "9 번째 페이지\n",
      "10 번째 페이지\n",
      "11 번째 페이지\n",
      "12 번째 페이지\n",
      "13 번째 페이지\n",
      "14 번째 페이지\n",
      "15 번째 페이지\n",
      "16 번째 페이지\n",
      "17 번째 페이지\n",
      "18 번째 페이지\n",
      "19 번째 페이지\n",
      "20 번째 페이지\n"
     ]
    }
   ],
   "source": [
    "## tkinter 관련\n",
    "from tkinter import *\n",
    "from tkinter.ttk import *\n",
    "from tkinter import scrolledtext\n",
    "from tkinter import messagebox\n",
    "import webbrowser\n",
    "\n",
    "## 데이터 pandas 관련\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "## sqlite\n",
    "import sqlite3 # sql에 저장\n",
    "\n",
    "\n",
    "data = pd.DataFrame(columns=['회사이름', '공고제목', '직무', '경력', '학력', '모집기간', '상세보기'])\n",
    "\n",
    "#### 웹 스크레핑\n",
    "page = 1\n",
    "while page <= 20:\n",
    "    url = 'https://www.jobkorea.co.kr/Starter/?JoinPossible_Stat=0&schOrderBy=0&LinkGubun=0&LinkNo=0&schType=0&schGid=0&Page={}'.format(\n",
    "        page)\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\"}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    res.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "    print('{} 번째 페이지'.format(page))\n",
    "    corporates = soup.find(\"div\", attrs={\"class\": \"filterListArea\"}).find_all(\"li\")\n",
    "\n",
    "    # 회사이름, 공고제목, 직무, 경력, 모집기간 추출\n",
    "    for i in range(len(corporates)):\n",
    "        try:\n",
    "            a = ','.join(corporates[i].find(\"a\", attrs={\"class\": \"coLink\"}).get_text().split())\n",
    "            b = ','.join(corporates[i].find(\"a\", attrs={\"class\": \"link\"}).get_text().split())\n",
    "            c = ','.join(corporates[i].find(\"div\", attrs={\"class\": \"sTit\"}).get_text().split())\n",
    "            d = ','.join(corporates[i].find(\"div\", attrs={\"class\": \"sDesc\"}).find('strong').get_text().split())\n",
    "            e = ','.join(corporates[i].find(\"div\", attrs={\"class\": \"sDesc\"}).find('span').get_text().split())\n",
    "            f = ','.join(corporates[i].find(\"span\", attrs={\"class\": \"day\"}).get_text().split())\n",
    "            g = \"https://www.jobkorea.co.kr\" + corporates[i].find(\"div\", attrs={\"class\":\"tit\"}).a['href']\n",
    "\n",
    "            alphas = [a, b, c, d, e, f, g]\n",
    "            data = data.append(pd.Series(alphas, index=data.columns), ignore_index=True)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    page += 1\n",
    "\n",
    "print('웹스크래핑 완료')\n",
    "con = sqlite3.connect('JOB_KOREA_FINAL.db') # 'JOB_KOREA_FINAL' 라는 이름의 DB 생성\n",
    "data.to_sql('Recruitment', con, index = False) # 'Recruitment' 라는 이름의 table 생성\n",
    "con.commit() # DB에 저장\n",
    "con.close()\n",
    "print('DB 생성')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
